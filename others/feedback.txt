why CWE
gists are just toy example, nor a real thing. problem will occur only when someone blindly copy it stress it enough
result looks like a lot alarming but actually it is not unless someone blindly copy it. stress it enough
sample size is less
cover this things in the limitation section
delete ref 1, 2, 3

The only drawback is a more rigorous evaluation of this method for some of the smells. For instance, in the case of  Bad File Permission, only one data point was used to evaluate the authors' approach, while in other cases 2 to 86 samples were used. 

why rq1 and rest of rqs are separate

Regarding RQ1: it is not clear from where the 13 security smells were
computed. For example, in the introduction, the authors state: "We first apply
qualitative analysis [7] on 5,822 Python Gists to find out what security
smells exist in Python Gists.". So, did the authors analyse all 5,8K Gists?
However, in Section III, which describes RQ1, there is no mention to
those 5,8K Gists. For example, the authors state in Section III: "We first 
obtain a list of insecure coding practices from these six sources", then
"Next, the first two authors of this paper individually map each of those 
identified smells to a potential security weakness indexed in CWE". 
Again, there is no mention to the 5,8K Gists reported in introduction. 
Please, make this point clearer in the paper.


The authors could better motivate the reputation score metric. I wonder
whether some other author metric based on the amount of commits or
authorship-like metrics would have been more effective. As a suggestion,
other author measures could be explored to relate with security smells.

Minors

Remove the dot after "example." in the second paragraph of the introduction.



